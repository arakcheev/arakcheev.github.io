---
layout: post
title: "Линейные модели. Регрессия."
author: "Artem"
category: ml
---

# Линейные модели. Регрессия. 

Идея этого поста в том, что бы разобрать одно из заданий курса машинного обучения на [Coursera.org](https://www.coursera.org) по линейной регрессии. Под словом разобрать я подразумеваю понимание постановки и решения задачи. Я не буду углубляться в тонкие детали посвященные машинному обучению, всевозможной настройке алгоритмов и тому подобное. Естественно, разобранный пример можно (и нужно) решать другим способом и многие представленные функции нельзя использовать "на продакшине". Представленный здесь код скорее нужен для базового понимания работы алгоритма линейной регрессии. Итак, начнем.  

Итак, работать будем с данными [Advertising.csv](http://www-bcf.usc.edu/~gareth/ISL/data.html), что представляют собой набор данных из книги *Introduction to Statistical Learning*. Данные нужно загрузить в Pandas Dataframe, что очень просто в python. Так как в таблице разделитель это запятая и заголовок уже присудствует, то никаких дополнительных параметров указывать не надо. 

```python
import pandas as pd
adver_data = pd.read_csv('Advertising.csv')
```

Далее, нужно посмотреть на первые 5 записей и статистику по признакам.  Первые 5 записей показать очень просто -  нужно вызвать метод `head` у обьекта DataFrame. В качестве аргумента можно передать число строк, которые нужно показать, которое по умолчанию равно 5.

```python
adver_data.head()
#Out:
#    TV	Radio	Newspaper	Sales
#1	230.1	37.8	69.2	22.1
#2	44.5	39.3	45.1	10.4
#3	17.2	45.9	69.3	9.3
#4	151.5	41.3	58.5	18.5
#5	180.8	10.8	58.4	12.9
```

Этим можно увидеть, что данные прочитались правильно и ожидаемые значения стоят в кажкой колонке. Вот насчет статистики, тут уже интереснее. Для начала, посмотрим сколько у нас вообще данных с помощью команды `shape`:

```python
adver_data.shape
#(200, 4)
```

200 строк и 4 переменные - для статистики это конечно маловато, но линейные (и не только) модели построить можно. Метод, который мне нравится для того, что бы наглядно увидеть возможную статистическую зависимость в данных - построение парных графиков. Сделать это удобно с помощью библиотеки [https://seaborn.pydata.org](https://seaborn.pydata.org) в которой есть метод `pairplot` который строит попарные зависимости признаков из датасета (признаки - это клонки). 

```python

import seaborn as sns;
import matplotlib.pyplot as plt

sns.pairplot(adver_data)
plt.show()
```

![pairplot.png]({{ "/assets/images/pairplot.png" }})

На диагонали показы распредения соответсвующего признака, например, колько ысего было *Sales* каждого вида. Из данных графиков уже можно сделать несколько интересных выводов по данным, касательно того, как влияет реклама в газетах, радио и рекламе на продажи. Видно, что меньше всего виляет реклама в газетах, потом в радио и наконец больше всего влияет реклама на ТВ. Далее, можно посмотреть кореляцию данных  с помощью простого метода `.corr()` в котором можно указать какую корреляцию считать - по умолчанию это корреляция [пирсона](http://www.machinelearning.ru/wiki/index.php?title=Коэффициент_корреляции_Пирсона), которая показывает существование линейной зависимости между величинами. Корреляция рекламы на ТВ и продажами порядка 78 процентов, далее идет радио с 57 процентами ну и наконец газеты с 22 процентами. 

```Python

#		      TV		Radio		Newspaper	Sales
#TV	        1.000000	0.054809	0.056648	0.782224
#Radio	    0.054809	1.000000	0.354104	0.576223
#Newspaper	0.056648	0.354104	1.000000	0.228299
#Sales	    0.782224	0.576223	0.228299	1.000000

```

Далее, в задании хотят отдельно выделить матрицу признаков и целевую переменную в отдельные массивы и затем их пронормировать. Я же хочу рассмотреть другой подход с которым нормировка получается такой же простой но и остается структура данных. Существует несколько способов нормировки данных и я хочу рассмотерть два из них: *нормировать на среднее и дисперсию,*  *и нормировать на отрезок [0, 1].* Рассмотрим сначала первый вариант: 

$$
x_{i} = \frac{x_{i} - \overline{x}}{\sigma} \\
\sigma - \text{дисперсия случайной величины x} 
$$

```python
for c in adver_data.columns: #итерируемся по всем признакам, включая Sales
    mean, std = adver_data[c].mean(), adver_data[c].std() #mean и std
    adver_data[c]=(adver_data[c] - mean)/std #нормируем данные
adver_data.head() # выводим первые 5 строчек что бы посмотреть что получилось

#	TV	          Radio	    Newspaper	Sales
#1	0.967425	0.979066	1.774493	1.548168
#2	-1.194379	1.080097	0.667903	-0.694304
#3	-1.512360	1.524637	1.779084	-0.905135
#4	0.051919	1.214806	1.283185	0.858177
#5	0.393196	-0.839507	1.278593	-0.215143
```

Видно, что величины уже лишены большого разброса (от 17 до 250 для рекламы в ТВ). Вообще, для выполнения такой нормировки есть специальный класс в пакете `sklearn.preprocessing`. 

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
pd.DataFrame(scaler.fit_transform(adver_data), columns=['TV', 'Radio', 'Newspaper', 'Sales']).head()

#	 TV	  			Radio	Newspaper	Sales
#0	0.969852	0.981522	1.778945	1.552053
#1	-1.197376	1.082808	0.669579	-0.696046
#2	-1.516155	1.528463	1.783549	-0.907406
#3	0.052050	1.217855	1.286405	0.860330
#4	0.394182	-0.841614	1.281802	-0.215683
```

Отлично, мы поняли как работает нормировка данных и `StandardScaler` работает также, как мы написали. Позднее мы рассмотрим другую нормировки и посмотрим, как они влияют на результат. 



> **Добавьте к матрице X столбец из единиц, используя методы hstack, ones и reshape библиотеки NumPy. Вектор из единиц нужен для того, чтобы не обрабатывать отдельно коэффициент w0 линейной регрессии.**

Следующим пунктом нас просят добавить еще один столбец в матрицу признаков и заполнить его единицами, что делается элементарно. 

```python
adver_data['bias'] = 1
adver_data.head(2)

#	 TV		Radio	Newspaper Sales	bias
#1	230.1	37.8	69.2	  22.1	1
#2	44.5	39.3	45.1	  10.4	1
```

> **Реализуйте функцию mserror - среднеквадратичную ошибку прогноза. Она принимает два аргумента - объекты Series y (значения целевого признака) и y_pred (предсказанные значения). Не используйте в этой функции циклы - тогда она будет вычислительно неэффективной.**  

Нужно реализовать среднеквадратичную ошибку не используя циклы в питоне (Это действительно логично делать без циклов). Нам нужно просусировать сумму квадратов отклонения предсказанного значения от реального и поделить все на количесво количесво данных. Так и напишем: 

```python
import numpy as np
def mserror(y, y_pred):
    return ( 
      		(y-y_pred)**2 # считаем квадраты отклонения 
           ).sum()/len(y_pred) # суммируем их и делим на длину вектора
    
print(mserror(np.array([1, 2, 3]), np.array([0.8, 1.97, 2.99])))
print(mserror(np.array([1, 2, 3]), np.array([0.9, 1.97, 2.99])))
print(mserror(np.array([1, 2, 3]), np.array([0.99, 1.97, 2.99])))
print(mserror(np.array([1, 2, 3]), np.array([0.99, 1.99, 2.99])))
print(mserror(np.array([1, 2, 3]), np.array([0.99999, 1.999999, 2.999999])))

#0.0136666666667
#0.00366666666667
#0.000366666666667
#0.0001
#3.39999999997e-11
```

Конечно же в питоне уже есть готовая реализация подсчета среднеквадратичной ошибки, и выглядит он просто:

```python
from sklearn.metrics import mean_squared_error

print(mean_squared_error(np.array([1, 2, 3]), np.array([0.99, 1.99, 2.99])))
#0.0001

```

что дает нам такое же значение ошибки. Едем далее.

> **Какова среднеквадратичная ошибка прогноза значений Sales, если всегда предсказывать медианное значение Sales по исходной выборке? Запишите ответ в файл '1.txt'.**

Пока все просто: вычисляем медианное значение продаж (это же 50 персентиль) и считаем среднеквадратичное отклонение:

```python
res1 = adver_data.Sales.quantile(q=0.50)
print("Mean sales ", res1)
answer1 = mserror(res1, adver_data.Sales)
print("Mean sales error ", answer1)
#Mean sales  -0.21514314223086323
#Mean sales error  1.04128657165
```

То, что ошибка получилась маленькая - **1.041286** - ничего не значит, так как мы пронормировали данные. Эту ошибку нужно сравнивать со среднеквадратичной ошибкой, колученной другим методом. 

> **Реализуйте функцию normal_equation, которая по заданным матрицам (массивам NumPy) X и y вычисляет вектор весов w согласно нормальному уравнению линейной регрессии.**

Так как задача линейной регрессии можем быть решена аналитически, то это аналитическое решение можно явно посчитать, что сейчас и сделаю. Итак, мы решаем уравнение $\vec{\textbf{X}}* \vec{\textbf{w}} \textbf{=} \vec{\textbf{y}}$ которое небходимо домножить на $\vec{\textbf{X}}^T$ что бы матрица перед весами была квадратная и можно было искать обратную матрицу (но не всегда это возможно). Т.е.  $\vec{X}^{T}\cdot\vec{X}\cdot \vec{w} = \vec{X}^T\cdot\vec{y}$  и матрица $\vec{X}^T\cdot\vec{X}$ теперь квадратная. 

Для решения уравнений вида $A\cdot x =b$ в питоне есть специальная функция **np.linalg.solve** которой нужно указать матрицу **А** и свободный вектор **b**. В нашем случае матрица **A** это  $\vec{X}^T\cdot\vec{X}$ а свободный вектор - $\vec{X}^T\cdot\vec{y}$. Таким образом, можно реализовать нашу функцию: 

```python
def normal_equation(X, y):
    a = np.dot(X.T, X) # преобразуем левую часть
    b = np.dot(X.T, y) # преобразуем правую часть
    res = np.linalg.solve(a, b) # решаем систему
    return res

FeatureMatrix = adver_data[['TV', 'Radio', 'Newspaper', 'bias']].values
TargetMatrix  = adver_data.Sales.values

norm_eq_weights= normal_equation(FeatureMatrix, TargetMatrix)
print(norm_eq_weights)

#[  7.53065912e-01   5.36481550e-01  -4.33068629e-03  -4.63419562e-17]
```

Вспомним корееляцию между признаками и видно, что коэфициенты очень походи на корреляцию. Теперь посчитаем какую ошибку мы получили, если будет предсказывать продажи через линейную модель. Для этого надо вектор признаков умножить на вектор весов. Реализуем сразу функцию для расчета предсказаний

```python
def linear_prediction(X, w):
    return np.dot(X, w)
 
y_pred = linear_prediction(FeatureMatrix, norm_eq_weights)
print(mserror(TargetMatrix, y_pred))
#0.102275415012
```

Значение ошибки уже меньше, чем предстказания по среднему на порядок. Но на самом деле это еще не предел.  

> **Какие продажи предсказываются линейной моделью с весами, найденными с помощью нормального уравнения, в случае средних инвестиций в рекламу по ТВ, радио и в газетах? (то есть при нулевых значениях масштабированных признаков TV, Radio и Newspaper).**

 Действительно, при такой нормировке данных, среднее значение будет нуль, иммено по этому вектор признаков из всех нулей - это вектор признаков средних значений признаков. 

```python
print(adver_data.TV.mean())
mean_values = np.array([0,0,0,1]).T 
print(linear_prediction(mean_values, norm_eq_weights), TargetMatrix.mean())
# 1.28785870857e-16
# -4.63419561864e-17 -1.86517468137e-16
```

Действительно, получилость так, что при средних значениях признаков, целевая переменная тоже принимает среденее значение. Едем дальше к градиетному спуску. 



> **5. Напишите функцию *stochastic_gradient_step*, реализующую шаг стохастического градиентного спуска для линейной регрессии. Функция должна принимать матрицу *X*, вектора *y* и *w*, число *train_ind* - индекс объекта обучающей выборки (строки матрицы *X*), по которому считается изменение весов, а также число *$\eta​$* (eta) - шаг градиентного спуска (по умолчанию *eta*=0.01). Результатом будет вектор обновленных весов. Наша реализация функции будет явно написана для данных с 3 признаками, но несложно модифицировать для любого числа признаков, можете это сделать.** 

Давайте сначала реализуем шаг градиентного спуска для каждой переменной, а затем запишем реализацию в общем. Но сначала опять вспомним, как решается задача линейной регресии. 

Идея линейной регресии заключается в том, что мы предполагаем, что наша целевая переменная, определенная на всем числовом пространстве, представима в виде линейной комбинации признаков $\tilde{y} = (\vec{w}, \vec{x})$ c весами $\vec{w}$. Зная, как представлен наш алгоритм для получения предсказания для целевой переменной мы можем записать функционал квадратичной ошибки: 

$$
Q(\vec{w}, \vec{x}, \vec{y}) = \frac{1}{N}\sum_{i=0}^{N}(\tilde{y}_i - y_i)^2 \rightarrow \min_{w}
$$

Так как функционал мы будем минимизировать по $\vec{w}$ то нам нужно найти такой способ изменения весов, что бы функционал ошибки уменьшался (по сути $\vec{w}$ это единственный параметр функционала, на который мы можем влиять и этот параметр напрямую влияет на наше решение и мы его оптимизируем).  И как раз один из таких способов изменения весов с целью минимизации функционала является градиент. Градиентом функции является вектор, составленый из частных производных по вектору пемеренных (в данном случае $\vec{w}$): 

$$
\bigtriangledown Q (\vec{w}) = (\frac{\partial Q}{w_0},\frac{\partial Q}{w_1},..., \frac{\partial Q}{w_n}).
$$

Градиент функционала ошибки показывает направление наискорейшего роста ошибки по параметрам весов, значит нам нужно менять веса в обратную сторону градиента:

$$
\vec{w} = \vec{w} - \eta*\bigtriangledown Q (\vec{w})
$$

тут $\eta$ - это гиперпарамтр обучения, который определяет шаг, с которым веса будут меняться. Идея в том, что градиент функции может быть очень большим и поэтому локальный минимум можно просутить, и болтатся вокруг него долго. Пожтому если уменьшить шаг градиента, то можно быстрее попасть в локальный минимум. Параметр $\eta$ как правило берут в диапазоне (0.01, 0.001, 0.1). Для случая линейной регресии градиент функции потерь можно легко посчитать (векторно и покомпонентно): 

$$
\bigtriangledown Q (\vec{w}) = \frac{2}{N}\sum_{i=0}^{N}(\tilde{y}_i - y_i)*\vec{x},\\
\bigtriangledown Q (\vec{w}) _i = \frac{2}{N}\sum_{i=0}^{N}(\tilde{y}_i - y_i)*x_i.
$$

Таким образом, выражение для обновления весов получается в виде 

$$
w_j = w_j - \frac{2\eta}{N}\sum_{i=0}^{N}(\tilde{y}_i - y_i)*x_j.
$$

Но, когда у нас очень большая матрица с данными ($N$ велико), то на каждом шаге пересчивать все значения не очень эффективно, поэтому используют так называемую **стохастическую реализацию градиентного спуска** в котором мы ссумирование по всем данным заменяем на случайный обьект из нашей марицы признаков: 

$$
w_j = w_j - \frac{2\eta}{N}(\tilde{y}_{k} - y_{k})*x_{jk},
$$

где $k$ - случайный индекс из матрицы признаков. 

Все, теперь можно написать реализацию шага градиентного спуска. 

```python
def stochastic_gradient_step(X, y, w, train_ind, eta=0.01):
    N = X.shape[0]                   # всего обьектов (нормировка)
    x = X[train_ind]                 # текуший случайный k обьект 
    y_pred = linear_prediction(x, w) # предсказание для к случайного обьекта 
    rs = (y_pred - y[train_ind])     # регрессионый остаток для k обьекта 
    
    grad0 = 2.0/N*x[0]*rs
    grad1 = 2.0/N*x[1]*rs
    grad2 = 2.0/N*x[2]*rs
    grad3 = 2.0/N*x[3]*rs
    return  w - eta * np.array([grad0, grad1, grad2, grad3])
```

 Или компактная запись 

```python
def stochastic_gradient_step_v(X, y, w, train_ind, eta=0.01):
    N = X.shape[0]                   
    x = X[train_ind]                 
    y_pred = linear_prediction(x, w) 
    rs = (y_pred - y[train_ind])     
    return  w - 2.0* eta/N*x*rs
```

Едем дальше.

> **6. Напишите функцию *stochastic_gradient_descent*, реализующую стохастический градиентный спуск для линейной регрессии. Функция принимает на вход следующие аргументы:**
>
> - X - матрица, соответствующая обучающей выборке
> - y - вектор значений целевого признака
> - w_init - вектор начальных весов модели
> - eta - шаг градиентного спуска (по умолчанию 0.01)
> - max_iter - максимальное число итераций градиентного спуска (по умолчанию 10000)
> - max_weight_dist - максимальное евклидово расстояние между векторами весов на соседних итерациях градиентного спуска, при котором алгоритм прекращает работу (по умолчанию 1e-8)
> - seed - число, используемое для воспроизводимости сгенерированных псевдослучайных чисел (по умолчанию 42)
> - verbose - флаг печати информации (например, для отладки, по умолчанию False)
>
> **На каждой итерации в вектор (список) должно записываться текущее значение среднеквадратичной ошибки. Функция должна возвращать вектор весов $w$, а также вектор (список) ошибок.**

У нас есть все параметры, что бы рассчитать новый вектор весов, ошибку функционала и принять решение - нужно ли дальше продолжать расчет. 

```python
def stochastic_gradient_descent(X, y, w_init, eta=1e-2, max_iter=1e4,
                                min_weight_dist=1e-8, seed=42, verbose=False):
    # Инициализируем расстояние между векторами весов на соседних
    # итерациях большим числом. 
    weight_dist = np.inf
    # Инициализируем вектор весов
    w = w_init
    # Сюда будем записывать ошибки на каждой итерации
    errors = []
    # Счетчик итераций
    iter_num = 0
    # Будем порождать псевдослучайные числа 
    # (номер объекта, который будет менять веса), а для воспроизводимости
    # этой последовательности псевдослучайных чисел используем seed.
    np.random.seed(seed)
        
    # Основной цикл
    while iter_num < max_iter:
        # порождаем псевдослучайный 
        # индекс объекта обучающей выборки
        random_ind = np.random.randint(X.shape[0])
        
        # Обновляем веса
        new_w = stochastic_gradient_step_v(X, y, w, random_ind, eta)
        
        # Считаем ошибку
        error = mserror(y, linear_prediction(X, new_w))
        errors.append(error)
        
        weight_dist = np.linalg.norm(w-new_w)
        w = new_w
        iter_num += 1
    print(weight_dist)            
    return w, errors
```

Обучим нашу модель

```python
%%time
stoch_grad_desc_weights, stoch_errors_by_iter =
  stochastic_gradient_descent(FeatureMatrix, TargetMatrix, [1,1,1,1],max_iter=1e5)             
print(stoch_grad_desc_weights)

#[  7.58819805e-01   5.31841619e-01  -8.72251139e-03  -3.86243052e-04]
```

Построим график как убывает среднеквадратичная ошибка

```python
plot(range(len(stoch_errors_by_iter)), stoch_errors_by_iter)
```

![График убывания квадратичной ошибки]({{"/assets/images/mse_by_iterations.png" | absolute_url}})

Интересно отметить, что значения получились почти такие же, как и в случае решения линейных уравнений. Примерно такой же получается ошибка: 0.10235771 против 0.102275. 

### Какие можно сделать выводы?

На самом деле все достататочно просто:

- линейные модели могут предсказывать нам целевую переменную с достататочно хорошей точностью и просто
- на больших обьемах данных стохастический градиентый спуск даст нам хорошее решение за небольшие вычислительные ресурсы

Но с другой стороны, линейные модели редко где используются. А когда понадобится использовать линейниые модели, то конечно лучше воспользоваться готовой реализацией, скжем из пакета *sklearn*

```python
from sklearn.linear_model import LinearRegression

regressor = LinearRegression()
regressor.fit(FeatureMatrix, TargetMatrix)
regressor.coef_

#[ 0.75306591,  0.53648155, -0.00433069,  0.        ]
```

