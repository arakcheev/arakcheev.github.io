Сейчас я хочу рассмотреть очень важный тип алгоритмов - **решающие деревья**. Как будет показано, решающие деревья и их композиции умеют находить достаточно сложные зависимости в данных удивительно простым способом. Я рассмотрю принцип построения решающего дерева, затем рассмотрю несколько вариантов композии деревьев, и напоследок разберу пару заданий из курса. 

## Решающее дерево 

Грубо говоря, алгоритм решающих деревьев был заимствован у людей, так как деревья похожи на то, как человек принимает решение. Например, кто то хочет купить новый телефон. Если сбережений на новый телефон больше, например, 50к рублей, то это новый iPhone. Если денег меньше, то можно спросить, хочет ли человек слетать в Тайланд? Ели хочет, то новый телефон Xiaomi, если не хочет, то это будет Samsung. На картинке видно такое простое дерево решения. 

![dt_example]({{site.url}}/assets/images/dt_example.png)

 Оно показывает логику, благодаря которой мы принимаем решения. Давайте попробуем эту логику как то формализировать, что бы, по возможности, научить машину делать подобное. В общем случае ограничимся бинарными деревьями (т.е. всего два ветвления будут - левое и правое). Начинать будем с самого верха корня дерева и иди в низ к листьям. Заметим, что в корне дерева мы сравниваем сумму наших сбережений с неким порогом - 50к рублей. Если сумма сбережений меньше порога, тогда мы идем в правую ветку, если больше то идем в левую. Если же мы пошли в право, то тут тоже можно сравнить значение с неким порогом. Действительно, хочет человек слетать в Тайланд можно описать бинарным признаком: 0 - не хочет, 1 - хочет. И тогда, наш порог будет выглядеть следующем образом: "хотелка" слетать в Тайланд <= 0. Таким образом можно сделать некий обобщающий вывод построения дерева решений :

1. Начинаем строить дерево от корня к листьям 
2. В каждой вершине сравниваем значение некоторого признака с неким порогом, получаем ветвление влево и вправо. 
3. Повторяем процедуру номер 2 

Алгоритм построения дерева машиной уже виднеется на горизонте, но пока остаются вопросы. **Как именно выбирать признак и порог? Когда останавливать построение дерева?** Попробуем сначала ответить на первый вопрос. Пусть у нас в какую то вершину $m$ попала подвыборка $X_m$. Нам нужно найти некий признак $j$ и порог этого признака $t$ , чтобы разбить $X_m \to X_l, X_r$. Как это можно сделать. Первое, что приходит в голову, надо мерить качество разбиения, т.е. нужна некоторая метрика, которая характеризовала объекты в $X_l, X_r$. Рассмотрим для определенности задачу классификации на два класса. Самая хорошая метрика разбивки $X_m \to X_l, X_r$ это когда в каждом подмножестве содержатся объекты только одного класса. Но это идеальный случай. В реальной задаче можно мерить разброс ответов в каждом подмножестве $X_l, X_r$ с помощью **критерия информативность**: 
$$
Q(X_m, j, t) = \frac{|X_l|}{|X_m|}\cdot H(X_l) + \frac{|X_r|}{|X_m|}\cdot H(X_r) \to \min_{j, t}
$$
Здесь $Q$ - метрика качества разбиения, которая чем меньше, тем лучше; $|X_i|$ - размер подмножества; $H(X_i)$ - метрика разброса ответов. Действительно, отношение размеров подмножества к исходному значению показывает значимость подмножества для метрики качества: если $|X_i| = 1$, то метрика разброса ответов $H(X_i)$ практически не важна.

 Как же можно мерить разброс ответов? Рассмотрим сначала задачу классификации и так называемый **критерий Джини** для разброса ответов. Пусть у нас $k$ классов всего. Тогда мы можем посчитать долю объектов $k$-го класса в разбиении: 
$$
p^i_k = \frac{\sum_j \left[ x_j \equiv k \right]}{|X_i|}
$$
$p^i_k$- доля объектов $k$-го класса в разбиении $i$. Тогда критерий Джини для метрики разброса ответов выглядит следующим образом: 
$$
H(X_i) = \sum^{\mathbb{K}}_{k=1} p_k^i\cdot(1-p_k^i).
$$
Если в подвыборку $X_i$ попали объекты только класса $k$, то $p_k^i \equiv 1$ и $p_k^i\cdot(1-p_k^i) \equiv 0$, тем самым уменьшая $Q$. Если распределение кассов равновероятно ($p_k^i = 0.5$) то критерий Джини будет максимальным. Вроде неплохая метрика качества разбиения. Есть еще энтропийная метрика разброса ответов: 
$$
H(X_i) = - \sum_{k=1}^{\mathbb{K}} p^i_k\cdot \ln p^i_k.
$$
Он показывает примерно тоже самое: если доля объектов к класса равна единица, то критерий минимальный. В общем случае такая метрика характеризует отличие разбиения от вырожденного, где энтропия равна нулю. Если же рассматривать задачу регрессии, то тут все проще: 
$$
H(X_i) = D[X_i] =  \frac{1}{|X_i|} \sum_i (y_i - \bar{y})^2 
$$
Это дисперсия ответов в подвыборке. Действительно, чем меньше дисперсия, тем лучше мы разбили.  

Хорошо, мы определились с метрикой качества разбиения, $Q(X_m, j, t)$, теперь надо понять, как выбирать $j, t$. Благо всего признаков в выборке не так много и можно делать перебор по всем $j$, что бы найти минимальное значение $Q$. С порогом тоже самая история. Зная как померить получившееся разбиение, перебираем все признаки $j$ и пороги $t$, оставляя только те $j, t$ для которых критерий информативности минимален. Тут стоит отметить, что порог $t$ для бинарных (и категориальных) признаков легко перебирается (все возможные значения) , а для вещественных можно брать точки между значениями признака в $X_m$.

Мы разобрались, как делать разбиение, теперь рассмотрим простой вопрос - когда нам остановится? Когда хватит ветвись дерево. Тут есть два варианта: (i) ограничить дерево по глубине (ii) разбивать до тех пор, пока в листе не останется $n$ объектов. Второй пункт понятен, нужно выбрать $n$ таким образом, что бы мы смогли сказать о распределении данных в листе. Как правило $n =5$. Первый же пункт как правило используется в композиции деревьев, о чем будет сказано ниже. Когда дерево остановить мы знаем, но как же предсказать результат теперь? На самом деле все по тупому просто. **Если это задача регрессии, то мы предсказываем среднее по объектам в листе**. Если это **задача классификации, то предсказываем самый популярный класс в листе.** Более того, частота этого класса и будет вероятностью, что очень неплохо. 

Таким образом, мы знаем все, что бы запрогать решающее дерево для задачи. Начинаем строить с корня до листьев (жадный алгоритм), перебираем признаки и пороги, что бы найти лучшее разбиение, останавливаемся по критерию останова и считает ответ. Казало бы, что еще нужно? Оказывается, что решающие деревья очень сильно переучиваются. И это очень большой минус. Рассмотрим подробнее, что происходит и как с этим можно бороться.

## Композиции деревьев. Случайный лес. 

Так как дерево может быть глубоким, то оно пытается уловить самые сложные зависимости. Например, на картинке показано как дерево пытается уловить тончайшие закономерности  

![dt_overfit]({{site.url}}/assets/images/dt_overfit.png)

и посреди "района" красных точек пытается восстановить зависимость для синих точек. Такое переобучение должно быть очевидным. Алгоритм не пытается найти закономерность в данных, а он подстраивается под них. Если такому решающему дереву дать тестовую выборку, то оно покажет плохие результаты. Но, есть одна особенность. Если мы чуть изменим выборку, а именно изначально выкинем некое подмножество исходной выборки и посмотрим, что у нас получится. 

![dt_overfit2]({{site.url}}/assets/images/dt_overfit2.png)

Видно, что разделяющая поверхность изменилась. Т.е. решающее дерево достаточно сильно меняется при изменении подборки. А что, если попробовать обратить эту особенность в преимущество? 

Я пропущу некие математические основания объединения алгоритмов в композиции. Лишь хочу упомянуть важные выводы. Ошибка предсказания алгоритма складывается из трех [величин](https://habrahabr.ru/company/ods/blog/323890/#razlozhenie-oshibki-na-smeschenie-i-razbros-bias-variance-decomposition): (i) шум (ii) смещение (iii) разброс. Шум это то, на что мы повлиять не сможем, это особенность данных. Смещение это матожидание ошибок, а разброс это дисперсия ошибок. Так вот оказывается, что для деревьев смещение очень мало, так как они сильно переучиваются на данных аппроксимируют сложные зависимости. А разброс ошибок тем ниже, чем меньше коррелированы предсказания различных деревьев. Таким образом, если мы обьеденим несколько деревьев в композицию, то смещение мало изменится, а разброс может стать существенно меньше. Рассмотрим, как можно это сделать. 

Так как дерево сильно меняется при изменении выборки, то что если начать генерить различные выборки из исходной и строить деревья не них? Из выборки длинной $M$ генерим подмножество длины $M$ c возвращением. Такой подход называется **Бутстрэп**. Например, для множества
$$
\{a_1, a_2, a_3, a_4, a_5\} \to  \{a_1, a_1, a_3, a_4, a_4\}
$$
И прочее похожие варианты. Для каждой такой выборки строим наше дерево $T_i(x)$ где $i$ номер выборки и тогда суммарный алгоритм будет иметь вид
$$
a(x) = \frac{1}{M}\sum_i^{M} T_i(x).
$$
Где $M$ число наших новых выборок. В случае классификации мы выбираем тот класс, за который больше всего проголосовало алгоритмов. Вот оказывается, что такой подход позволяет уменьшить разброс ошибки в $M$ раз! **Это называется Бэггинг.** Достигается это благодаря малой корреляции деревьев $T_i$. 

Супер, мы знаем один способ обьединить деревья в композиции - это беггинг (на самом деле это будет работать не только с деревьями). Но, на самом деле можно еще уменьшить коррелированность деревьев, что бы уменьшить разброс. Что если при построении дерева мы будем не перебирать все признаки, а выбирать из из некого случайного подмножества признаков $q$? Т.е. используем тот-же самый беггинг, только добавляем элемент случайности в построение дерева. Построенный таким образом алгоритм называют **случайным лесом или random forest.** Это наверно самый популярный алгоритм в машинном обучении (разные люди [пишут](https://alexanderdyakonov.wordpress.com/2016/11/14/случайный-лес-random-forest/), что порядка 70% задач решается с помощью лесов). У RF практически отсутствуют гиперпараметры - как правило это кол-во деревьев и кол-во случайных признаков. Да, это очевидно, что деревья можно строить параллельно, так как построения вообще не связаны друг с другом. Это вообще потрясающе. Можно брать и пользоваться, но что нам мешает остановиться? Но есть еще один подход построения композиции. 

## Градиентный бустинг. 

Хотя RF строится параллельно, но на больших данных и при большом кол-ве признаков строить очень глубокие деревья не очень эффективно. Процесс получается очень долгий. Мы можем ускорить построение деревьев, ограничив глубину, но тогда ты теряем в точности, так как смещение композиции алгоритмов уже не обязано быть маленьким. Кроме того, для решения сложных задач может потребоваться больше деревьев, чем хотелось бы. Как именно определять необходимое количество деревьев? На самом деле, если мы откажемся от позиции, где каждое дерево строится независимо от всех остальных и попытаемся учитывать "опыт" прошлых деревьев, то можно решить эти проблемы. Например, пусть мы построили дерево первое дерево, которое дало предсказание нашей целевой переменной $y_1$. Тогда ошибка предсказания будет 
$$
\varepsilon_1 = y - y_1
$$
Давайте теперь построим следующее дерево номер два таким образом, чтобы оно минимизировало 